{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NbBhBneekPS"
      },
      "source": [
        "<img src=\"https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371573952659\">\n",
        "\n",
        "---\n",
        "\n",
        "# WEB ANALYTICS COURSE 4 - SEMESTER 2\n",
        "# BACHELOR IN DATA SCIENCE AND ENGINEERING\n",
        "\n",
        "# LAB 1.2 WEB SCRAPING WITH SCRAPY\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuSPOB1SjLa7"
      },
      "source": [
        "## **Participants:**\n",
        "\n",
        "Beltran Valero 100451816\n",
        "\n",
        "Annunziata Alvaréz-Cascos 100451939\n",
        "\n",
        "Gracia Estrán 100452014\n",
        "\n",
        "Marta Almagro 100451979"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzkYIJSFemAu"
      },
      "source": [
        "# 0. Lab Preparation\n",
        "\n",
        "1.  Study and have clear the concepts explained in the theoretical class and the introductory lab.\n",
        "\n",
        "2.   Gain experience with the use of the [Scrapy](https://scrapy.org/). The exercises of this lab will be mainly based on the utilization of functions offered by this library.\n",
        "\n",
        "3. It is assumed students have experience in using Python notebooks. Either a local installation (e.g., local python installation + Jupyter) or a cloud-based solution (e.g., Google Colab). *We recommend the second option*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsNp9tDLemJi"
      },
      "source": [
        "# 1. Lab Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8PD5y3wemP8"
      },
      "source": [
        "* In this lab, we will implement a web scraper using [Scrapy](https://scrapy.org/). One of the tools explained in the theoretical class.\n",
        "\n",
        "* The lab will be done in groups of 4 people.\n",
        "\n",
        "* The lab defines a set of milestones the students must complete. Upon completing all the milestones, students should call the professor, who will check the correctness of the solution (*If the professor is busy, do not wait for them, move to the next lab*).\n",
        "\n",
        "* **The final mark will be computed as a function of the number of milestones successfully completed.**\n",
        "\n",
        "* **Each group should also share their lab notebook with the professor upon the finalization of the lab.**\n",
        "\n",
        "* In this lab we will use the [Scrapy](https://scrapy.org/) library for the creation of a web scraper, to extract information from the web. As indicated in the *Lab Preparation* section above, it is expected that students have gained experience in the use of the library before starting the first session of the lab.\n",
        "\n",
        "- It is recommended to use [Google Colab](https://colab.research.google.com/) to produce the Python notebook with the solution of the lab. Of course, if any student prefers using its local programming environment (e.g., jupyter) and python installation, they are welcome to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VT7rdy1emUL"
      },
      "source": [
        "# MILESTONE 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLgzJC8-eIuN",
        "outputId": "b073fa68-bca5-4e5a-d523-88ec798b57c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted<23.8.0,>=18.9.0 (from scrapy)\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-3.6.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.15.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n",
            "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-3.6.0 w3lib-2.1.2 zope.interface-6.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install scrapy # Install scarpy at the beginning each time we want to run the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr-Y_L4ygjMd"
      },
      "source": [
        "### a) Create a _Scrapy_ project for this lab (**HINT:** Remember you can use the command `startproject`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkD1o1NLeL-G",
        "outputId": "214ed930-13f3-4152-b7f6-5ebb5c3d9f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New Scrapy project 'SpiderProject', using template directory '/usr/local/lib/python3.10/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/drive/My Drive/WebAnalytics/SpiderProject_root\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd /content/drive/My Drive/WebAnalytics/SpiderProject_root\n",
            "    scrapy genspider example example.com\n",
            "__init__.py  items.py  middlewares.py  pipelines.py  settings.py  spiders\n"
          ]
        }
      ],
      "source": [
        "import scrapy # Import it \n",
        "!scrapy startproject SpiderProject '/content/drive/My Drive/WebAnalytics/SpiderProject_root' # Created a spider project in the folder SpiderProject_root\n",
        "!ls '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # To check what it is inside that folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke8Qn0eqgmOu"
      },
      "source": [
        "### b) Create a crawler/spider to the website [BACHELOR IN DATA SCIENCE AND ENGINEERING](https://www.uc3m.es/bachelor-degree/data-science).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgNM7dxufG0P",
        "outputId": "1ae7be9a-52fe-4b8d-bc11-28f8d0dac9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "Created spider 'My_spider' using template 'basic' in module:\n",
            "  SpiderProject.spiders.My_spider\n",
            "__init__.py  My_spider.py  __pycache__\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy genspider My_spider 'https://www.uc3m.es/bachelor-degree/data-science' # Create the crawler\n",
        "!ls '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject/spiders'\n",
        "# ^^^  To check that is creating in the correct place the spider python file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVJ2I4d0D9Hh"
      },
      "source": [
        "Now what we have to modify the My_spider python file and creating the class MySpider, where we will implement the texts of information we want to scrape.\n",
        "For example, to see if our crawler works we have programmed it so they get us the title and the information from the 'presentation' div of our degree page.\n",
        "\n",
        "####This code is the one written in the My_spider.py file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjvjEtU1HkK8"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"MySpider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the div with id=\"presentacion\"\n",
        "        div_presentacion = response.xpath('//div[@id=\"presentacion\"]')\n",
        "        presentation_title = ' '.join(div_presentacion.xpath('.//text()').getall()).strip()\n",
        "        # Print it in the log\n",
        "        self.log(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Presentation Title: \" + presentation_title)\n",
        "\n",
        "        # Locate the class 'col span_12' (which we know is that one because we\n",
        "        # opened the developer tools in our browser and locate it with that name)\n",
        "        span_12_presentation = response.xpath('//div[@class=\"col span_12\"]')\n",
        "        presentation_text = ' '.join(span_12_presentation.xpath('.//p/text()').getall()).strip()\n",
        "        # Print it in the log\n",
        "        self.log(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Presentation Text : \" + presentation_text)\n",
        "\n",
        "    # The separators ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> are put so we can visualize\n",
        "    # faster in the log the information we are printing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIQowlqUGyjF"
      },
      "source": [
        "As we modified the file we have to upload again the drive and executed from the right directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7B9SKFOUsHZ",
        "outputId": "f3b99348-dec3-4a93-da7e-3e4e86a4ff88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "2023-09-25 17:21:46 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: SpiderProject)\n",
            "2023-09-25 17:21:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "2023-09-25 17:21:46 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2023-09-25 17:21:46 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2023-09-25 17:21:46 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2023-09-25 17:21:46 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2023-09-25 17:21:46 [scrapy.extensions.telnet] INFO: Telnet Password: fa36fa094a3a08f3\n",
            "2023-09-25 17:21:46 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:21:46 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'SpiderProject',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'SpiderProject.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['SpiderProject.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2023-09-25 17:21:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:21:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:21:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:21:46 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:21:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:21:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-09-25 17:21:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2023-09-25 17:21:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2023-09-25 17:21:48 [MySpider] DEBUG: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Presentation Title: Presentation\n",
            "2023-09-25 17:21:48 [MySpider] DEBUG: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Presentation Text : The world of the 21st century generates massive amounts of data and, therefore, urgently needs experts capable of extracting meaning from them and putting them into value. The Degree in Science and Data Engineering will train professionals with the ability to analyze, both theoretically and practically, said data for intelligent decision making. If you are a person with analytical skills, critical thinking, computer skills and mathematical skills, this degree will prepare you to generate practical solutions to technological, business and social problems. Combine the study of fundamental subjects such as mathematics or computer science, with the new tools coming from the digital technologies of information and communication, including statistics, artificial intelligence or machine learning. In short, the Degree will turn you into a leader of the fourth industrial revolution. UC3M has agreements with over 3000 companies and institutions in which students can undertake internships and access job openings. A total of 93.4 % of graduates from this University enter the job market the first year after finishing their studies, according to the 2019  .\n",
            "2023-09-25 17:21:48 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:21:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 16880,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.276065,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 21, 48, 64570, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 77520,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 7,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 21, 46, 788505, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:21:48 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive') # Upload the drive\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy crawl MySpider # Run MySpider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGbwB8rIgo9X"
      },
      "source": [
        "### c) Add the code to the crawler to get PROGRAM header. **TIP:** Find the element tag with `id=\"program\"` and print the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq4_DHqpfHBb"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"MySpider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the div with id=\"program\"\n",
        "        program_div = response.xpath('//div[@id=\"program\"]')\n",
        "        program_header = program_div.xpath('.//h2/text()').get()\n",
        "        # Print it in the log\n",
        "        self.log(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Program Header: \" + program_header.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkDfGwwdI7gL",
        "outputId": "5cd1641a-fb42-492f-9b1f-fd01e70b2ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "2023-09-25 17:22:25 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: SpiderProject)\n",
            "2023-09-25 17:22:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "2023-09-25 17:22:25 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2023-09-25 17:22:25 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2023-09-25 17:22:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2023-09-25 17:22:25 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2023-09-25 17:22:25 [scrapy.extensions.telnet] INFO: Telnet Password: f58221cec99b6aab\n",
            "2023-09-25 17:22:25 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:22:25 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'SpiderProject',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'SpiderProject.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['SpiderProject.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2023-09-25 17:22:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:22:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:22:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:22:25 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:22:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:22:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-09-25 17:22:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2023-09-25 17:22:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2023-09-25 17:22:26 [MySpider] DEBUG: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Program Header: Program\n",
            "2023-09-25 17:22:26 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:22:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 16879,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.28142,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 22, 26, 498954, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 77520,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 6,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 22, 25, 217534, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:22:26 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive') # Upload the drive\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy crawl MySpider # Run MySpider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wNkZc1ghFcS"
      },
      "source": [
        "### d) Add the code to the crawler to find the table inside PROGRAM for Course 1 - Semester 1 and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LEObFkUnIHx"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"MySpider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "            \n",
        "            # Locate the div with id=\"program\"\n",
        "            program_div = response.xpath('//div[@id=\"program\"]')\n",
        "            program_header = program_div.xpath('.//h2/text()').get()\n",
        "            # Print it in the log the header\n",
        "            self.log(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Program Header: \" + program_header.strip())\n",
        "\n",
        "            course1_semester1_table = program_div.xpath('//h2[contains(text(), \"Curso 1 - Cuatrimestre 1\")]/following-sibling::table[1]')\n",
        "\n",
        "            rows = course1_semester1_table.xpath('.//tr') # Extract table rows\n",
        "\n",
        "            # Iterate through rows and print the content\n",
        "            for row in rows:\n",
        "                columns = row.xpath('.//td')\n",
        "                data = [col.xpath('string(.)').get().strip() for col in columns]\n",
        "                self.log(\"Table Row Data: \" + \" | \".join(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLh8ez5VJKvV"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive') # Upload the drive\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy crawl MySpider # Run MySpider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bza8BE2qJUBm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osxY_uWdf-WP"
      },
      "source": [
        "# MILESTONE 2\n",
        "\n",
        "a) Obtain the link to Web Analytics course by finding the corresponding href.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfe0Ood3XiuL"
      },
      "source": [
        "### This code is the one written in the My_spider1.py file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55BG8TrRsa7Z"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"MySpider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the link with text=\"Web Analytics\"\n",
        "        web_analytics_link = response.xpath('//a[contains(text(), \"Web Analytics\")]/@href').get()\n",
        "\n",
        "        # Print it in the log\n",
        "        self.log(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Web Analytics Link: \" + web_analytics_link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iChuT4XrW6A",
        "outputId": "6ef9eaf9-e354-41d8-b37a-84c128dc8db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "2023-09-25 17:27:30 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: SpiderProject)\n",
            "2023-09-25 17:27:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "2023-09-25 17:27:30 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2023-09-25 17:27:30 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2023-09-25 17:27:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2023-09-25 17:27:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2023-09-25 17:27:30 [scrapy.extensions.telnet] INFO: Telnet Password: f1acdc8033cc5cf0\n",
            "2023-09-25 17:27:30 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:27:30 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'SpiderProject',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'SpiderProject.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['SpiderProject.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2023-09-25 17:27:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:27:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:27:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:27:30 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:27:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:27:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-09-25 17:27:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/robots.txt> (referer: None)\n",
            "2023-09-25 17:27:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2023-09-25 17:27:32 [MySpider] DEBUG: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Web Analytics Link: https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\n",
            "2023-09-25 17:27:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:27:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 462,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 16846,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.318017,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 27, 32, 206336, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 77520,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 6,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 27, 30, 888319, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:27:32 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive') # Upload the drive\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy crawl MySpider # Run MySpider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lACe-J0PsbZw"
      },
      "source": [
        "b) Create a new spider _class_ and access to this URL.\n",
        "\n",
        "**TIP**: For this milestone, you need to create a new crawler and give it a different name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evIbljlFsauY",
        "outputId": "ede483e7-bb77-4224-b03e-437d1f4b1494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "Created spider 'My_spider1' using template 'basic' in module:\n",
            "  SpiderProject.spiders.My_spider1\n",
            "__init__.py  My_spider1.py  My_spider.py  __pycache__\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy genspider My_spider1 'https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2' # Create the secon crawler\n",
        "!ls '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject/spiders' # Check there is another python file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf0HvVPHsb8o"
      },
      "source": [
        "c) Print the text inside the _Description of contents: programme_ section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHV_yJPbsanK"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "\n",
        "class MySpider1(scrapy.Spider):\n",
        "    name = \"MySpider1\"\n",
        "    allowed_domains = [\"aplicaciones.uc3m.es\"]\n",
        "    start_urls = [\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "        # Get the div that contains the Description title\n",
        "        title_div = response.xpath('//div[@class=\"panel-heading degradado\" and contains(text(), \"Description of contents: programme\")]')\n",
        "\n",
        "        # Extract the following sibling div that contains the description\n",
        "        description_div = title_div.xpath('./following-sibling::div[1]')\n",
        "\n",
        "        # Extract the text inside the description div\n",
        "        description_text = description_div.xpath('.//text()').getall()\n",
        "\n",
        "        # Join and print in the log\n",
        "        description = ''.join(description_text).strip()\n",
        "        self.log(\"Description of Contents: \" + description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7PV8ENOsogf",
        "outputId": "aca3f0d4-5451-4c5c-91cc-0dd2436bf92d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "2023-09-25 17:30:19 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: SpiderProject)\n",
            "2023-09-25 17:30:19 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "2023-09-25 17:30:19 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2023-09-25 17:30:19 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2023-09-25 17:30:19 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2023-09-25 17:30:19 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2023-09-25 17:30:19 [scrapy.extensions.telnet] INFO: Telnet Password: cc292c4177d42d4d\n",
            "2023-09-25 17:30:19 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:30:19 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'SpiderProject',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'SpiderProject.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['SpiderProject.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2023-09-25 17:30:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:30:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:30:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:30:19 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:30:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:30:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-09-25 17:30:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/robots.txt> (referer: None)\n",
            "2023-09-25 17:30:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n",
            "2023-09-25 17:30:21 [MySpider1] DEBUG: Description of Contents: 1.\tData Collection in the Web ecosystem: \n",
            "        1.1  Scrapers, Crawlers\n",
            "        1.2\tAPIs\n",
            "2.\tData Analytics in the web\n",
            "        2.1  Graph Analysis: Centrality and Influence metrics\n",
            "        2.2  Network structure: \n",
            "               2.2.1 Type of networks (bipartite graph, small world, scale free)\n",
            "               2.2.2 Clustering, Community Detection, K-core decomposition\n",
            "\n",
            "3.\tWeb data visualization\n",
            "        3.1\tRepresentation of web information.\n",
            "        3.2\tVisualization tools.\n",
            "\n",
            "4.\tFinal Web Analytics Project\n",
            "        4.1\tThe project needs to include the three components presented above (Data Collection, Data Analytics and Data Visualization\n",
            "2023-09-25 17:30:21 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:30:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 505,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 7751,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.866563,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 30, 21, 755982, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 17368,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 6,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 30, 19, 889419, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:30:21 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive') # Upload the drive\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject' # Get the working directory\n",
        "!scrapy crawl MySpider # Run MySpider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cELeRKOf-ep"
      },
      "source": [
        "# MILESTONE 3\n",
        "\n",
        "a) Modify your code from previous milestones for running both crawlers in the same command line process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izdSl3zhG7Hx"
      },
      "outputs": [],
      "source": [
        "# Milestone1 (My_spider) # AQUI HABRIA QUE METER EL CODIGO DEL MILESTONE1 APARTADO D\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"MySpider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the div with id=\"program\"\n",
        "        program_div = response.xpath('//div[@id=\"program\"]')\n",
        "        program_header = program_div.xpath('.//h2/text()').get()\n",
        "        # Print it in the log\n",
        "        self.log(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Program Header: \" + program_header.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4GCC8g1Ml4c"
      },
      "outputs": [],
      "source": [
        "# Milestone2 (My_spider1)\n",
        "\n",
        "class MySpider1(scrapy.Spider):\n",
        "    name = \"MySpider1\"\n",
        "    allowed_domains = [\"aplicaciones.uc3m.es\"]\n",
        "    start_urls = [\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "\n",
        "        # Get the div that contains the Description title\n",
        "        title_div = response.xpath('//div[@class=\"panel-heading degradado\" and contains(text(), \"Description of contents: programme\")]')\n",
        "\n",
        "        # Extract the following sibling div that contains the description\n",
        "        description_div = title_div.xpath('./following-sibling::div[1]')\n",
        "\n",
        "        # Extract the text inside the description div\n",
        "        description_text = description_div.xpath('.//text()').getall()\n",
        "\n",
        "        # Join and print in the log\n",
        "        description = ''.join(description_text).strip()\n",
        "        self.log(\"Description of Contents: \" + description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SczAsEKMlyo"
      },
      "outputs": [],
      "source": [
        "# Spiders file\n",
        "\n",
        "import scrapy\n",
        "import sys\n",
        "\n",
        "# Add the path of our Scrapy project directory\n",
        "scrapy_project_path = '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject'\n",
        "sys.path.append(scrapy_project_path)\n",
        "\n",
        "# Import the classes from each file\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from spiders.My_spider import MySpider\n",
        "from spiders.My_spider1 import MySpider1\n",
        "\n",
        "# Call them in a process\n",
        "if __name__ == \"__main__\":\n",
        "    process = CrawlerProcess()\n",
        "    process.crawl(MySpider)\n",
        "    process.crawl(MySpider1)\n",
        "    process.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfEIxFKENR--",
        "outputId": "72326eb3-879e-4038-e8fb-363ad5a2a488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject/spiders\n",
            "2023-09-25 17:31:11 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: scrapybot)\n",
            "2023-09-25 17:31:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "2023-09-25 17:31:11 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "2023-09-25 17:31:11 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2023-09-25 17:31:11 [scrapy.extensions.telnet] INFO: Telnet Password: 2f24dce5f52eb947\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:31:12 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:31:12 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:31:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:31:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-09-25 17:31:12 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "2023-09-25 17:31:12 [scrapy.extensions.telnet] INFO: Telnet Password: b95b0149c3ebb840\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:31:12 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:31:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:31:12 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:31:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:31:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
            "2023-09-25 17:31:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.uc3m.es/bachelor-degree/data-science> (referer: None)\n",
            "2023-09-25 17:31:13 [MySpider] DEBUG: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~> Web Analytics Link: https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\n",
            "2023-09-25 17:31:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:31:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 240,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 15936,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 1.172525,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 31, 13, 371423, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 72019,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 3,\n",
            " 'log_count/INFO': 20,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 31, 12, 198898, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:31:13 [scrapy.core.engine] INFO: Spider closed (finished)\n",
            "2023-09-25 17:31:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n",
            "2023-09-25 17:31:13 [MySpider1] DEBUG: Description of Contents: 1.\tData Collection in the Web ecosystem: \n",
            "        1.1  Scrapers, Crawlers\n",
            "        1.2\tAPIs\n",
            "2.\tData Analytics in the web\n",
            "        2.1  Graph Analysis: Centrality and Influence metrics\n",
            "        2.2  Network structure: \n",
            "               2.2.1 Type of networks (bipartite graph, small world, scale free)\n",
            "               2.2.2 Clustering, Community Detection, K-core decomposition\n",
            "\n",
            "3.\tWeb data visualization\n",
            "        3.1\tRepresentation of web information.\n",
            "        3.2\tVisualization tools.\n",
            "\n",
            "4.\tFinal Web Analytics Project\n",
            "        4.1\tThe project needs to include the three components presented above (Data Collection, Data Analytics and Data Visualization\n",
            "2023-09-25 17:31:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:31:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 274,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 7401,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 1.458563,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 31, 13, 670698, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 17246,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/INFO': 13,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 31, 12, 212135, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:31:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject/spiders'\n",
        "!python /content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject/spiders/Spiders.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPAw6W4bsogx"
      },
      "source": [
        "b) Instead of printing the results (from Milestone 1 and 2), save them in a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHhZB0p8splB"
      },
      "outputs": [],
      "source": [
        "# MILESTONE 1\n",
        "import scrapy\n",
        "class MySpider(scrapy.Spider):\n",
        "    name = \"My_Spider\"\n",
        "    allowed_domains = [\"www.uc3m.es\"]\n",
        "    start_urls = [\"https://www.uc3m.es/bachelor-degree/data-science\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Locate the div with id=\"program\"\n",
        "        program_div = response.xpath('//div[@id=\"program\"]')\n",
        "        program_header = program_div.xpath('.//h2/text()').get()\n",
        "\n",
        "        # Save the program header to a file named \"output\"\n",
        "        with open(\"Output1\", \"w\") as output_file:\n",
        "            output_file.write(program_header.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qNndCpYRdIR"
      },
      "outputs": [],
      "source": [
        "# MILESTONE 2\n",
        "import scrapy\n",
        "class MySpider1(scrapy.Spider):\n",
        "    name = \"My_spider1\"\n",
        "    allowed_domains = [\"aplicaciones.uc3m.es\"]\n",
        "    start_urls = [\"https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Get the div that contains the Description title\n",
        "        title_div = response.xpath('//div[@class=\"panel-heading degradado\" and contains(text(), \"Description of contents: programme\")]')\n",
        "\n",
        "        # Extract the following sibling div that contains the description\n",
        "        description_div = title_div.xpath('./following-sibling::div[1]')\n",
        "\n",
        "        # Extract the text inside the description div\n",
        "        description_text = description_div.xpath('.//text()').getall()\n",
        "\n",
        "        # Join and save the description to the \"output\" file\n",
        "        description = ''.join(description_text).strip()\n",
        "        with open(\"Output2\", \"w\") as output_file:\n",
        "            output_file.write(description)\n",
        "\n",
        "        #self.log(\"Description of Contents saved to the 'output' file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jg5IFUFZiKH"
      },
      "source": [
        "Each of the milestones will create different outputs files with each information scraped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZwSa23nSO2j",
        "outputId": "e6874098-89af-410a-b5b2-9f55af8a35e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject\n",
            "2023-09-25 17:06:46 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: SpiderProject)\n",
            "2023-09-25 17:06:46 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "2023-09-25 17:06:46 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2023-09-25 17:06:46 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2023-09-25 17:06:46 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2023-09-25 17:06:46 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2023-09-25 17:06:46 [scrapy.extensions.telnet] INFO: Telnet Password: 2aa86e74cc3458bb\n",
            "2023-09-25 17:06:47 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2023-09-25 17:06:47 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'SpiderProject',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'SpiderProject.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['SpiderProject.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2023-09-25 17:06:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2023-09-25 17:06:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2023-09-25 17:06:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2023-09-25 17:06:47 [scrapy.core.engine] INFO: Spider opened\n",
            "2023-09-25 17:06:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2023-09-25 17:06:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2023-09-25 17:06:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/robots.txt> (referer: None)\n",
            "2023-09-25 17:06:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aplicaciones.uc3m.es/cpa/generaFicha?&est=350&plan=392&asig=16507&idioma=2> (referer: None)\n",
            "2023-09-25 17:06:48 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2023-09-25 17:06:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 505,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 7751,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 1.568997,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 9, 25, 17, 6, 48, 722742, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 17368,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'log_count/DEBUG': 5,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 144498688,\n",
            " 'memusage/startup': 144498688,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 9, 25, 17, 6, 47, 153745, tzinfo=datetime.timezone.utc)}\n",
            "2023-09-25 17:06:48 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/WebAnalytics/SpiderProject_root/SpiderProject'\n",
        "#!scrapy crawl MySpider\n",
        "!scrapy crawl MySpider1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
